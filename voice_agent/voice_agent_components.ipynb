{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a37bb7e-bae5-4af4-883b-24f134524772",
   "metadata": {
    "id": "WMGxBP-yQoCl"
   },
   "source": [
    "# Voice Agent Components\n",
    "\n",
    "This notebook demonstrates the implementation of a sophisticated voice-enabled AI agent using LiveKit Agents. The agent combines state-of-the-art speech recognition, language understanding, and voice synthesis to create natural, interactive voice conversations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426c986e-7277-4dd9-908f-875fbc33d9d4",
   "metadata": {},
   "source": [
    "## Setup and Configuration\n",
    "\n",
    "Importing required LiveKit Agent modules and plugins for voice interaction capabilities. This includes:\n",
    "- OpenAI for LLM and Speech-to-Text\n",
    "- ElevenLabs for Text-to-Speech\n",
    "- Silero for Voice Activity Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512fe011-1747-495b-9351-286b4e2f5c53",
   "metadata": {
    "height": 283
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "_ = load_dotenv(override=True)\n",
    "\n",
    "logger = logging.getLogger(\"dlai-agent\")\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "from livekit import agents\n",
    "from livekit.agents import Agent, AgentSession, JobContext, WorkerOptions, jupyter\n",
    "from livekit.plugins import (\n",
    "    openai,\n",
    "    elevenlabs,\n",
    "    silero,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc94cc2-4301-48aa-a205-0a0146f6cc0c",
   "metadata": {},
   "source": [
    "## Step 2: Define Your Custom Agent\n",
    "\n",
    "## Voice Assistant Implementation\n",
    "\n",
    "The Assistant class implements a voice-enabled AI agent with the following components:\n",
    "\n",
    "- **Speech-to-Text (STT)**: OpenAI Whisper for accurate transcription\n",
    "- **Language Model (LLM)**: GPT-4 for natural language understanding\n",
    "- **Text-to-Speech (TTS)**: ElevenLabs for high-quality voice synthesis\n",
    "- **Voice Activity Detection**: Silero VAD for precise utterance detection\n",
    "\n",
    "The agent can be customized with different voices and personalities through configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e00efd4-d48d-4dac-b629-ab6a4c198b73",
   "metadata": {
    "height": 334
   },
   "outputs": [],
   "source": [
    "class Assistant(Agent):\n",
    "    def __init__(self, voice_id: str = None, instructions: str = None) -> None:\n",
    "        \"\"\"Initialize the voice assistant with configurable voice and instructions.\n",
    "        \n",
    "        Args:\n",
    "            voice_id (str, optional): ElevenLabs voice ID for synthesis\n",
    "            instructions (str, optional): Custom instructions for the agent\n",
    "        \"\"\"\n",
    "        llm = openai.LLM(model=\"gpt-4o\")\n",
    "        stt = openai.STT(model=\"whisper-1\")\n",
    "        tts = elevenlabs.TTS(voice_id=voice_id) if voice_id else elevenlabs.TTS()\n",
    "        silero_vad = silero.VAD.load()\n",
    "\n",
    "        super().__init__(\n",
    "            instructions=instructions or \"\"\"\n",
    "                You are a helpful and professional voice assistant.\n",
    "                Communicate clearly and naturally, maintaining a friendly tone.\n",
    "                Provide concise yet informative responses.\n",
    "                When appropriate, ask clarifying questions.\n",
    "            \"\"\",\n",
    "            stt=stt,\n",
    "            llm=llm,\n",
    "            tts=tts,\n",
    "            vad=silero_vad,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be129825-3a95-4e8f-b3a1-ef0cbac8b7b5",
   "metadata": {},
   "source": [
    "## Step 3: Create the Entrypoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5dbcacb-b143-46d5-a649-27c0eb3b5cf6",
   "metadata": {
    "height": 166
   },
   "outputs": [],
   "source": [
    "async def entrypoint(ctx: JobContext):\n",
    "    await ctx.connect()\n",
    "\n",
    "    session = AgentSession()\n",
    "\n",
    "    await session.start(\n",
    "        room=ctx.room,\n",
    "        agent=Assistant()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526ddb2c-cc9b-4a92-80d9-1f0a56532f69",
   "metadata": {},
   "source": [
    "## Usage Instructions\n",
    "\n",
    "1. **Starting the Voice Interaction**:\n",
    "   - Click the microphone icon on the left to enable voice input\n",
    "   - The 'Start Audio' button can be ignored\n",
    "\n",
    "2. **Language Detection**:\n",
    "   - The agent automatically detects the language you speak\n",
    "   - For optimal detection, begin with a complete sentence\n",
    "   - Example: \"Hello, how are you today?\"\n",
    "\n",
    "3. **Voice Customization**:\n",
    "   - Different voice personalities are available through ElevenLabs\n",
    "   - See the voice options section below for available voices\n",
    "\n",
    "4. **Best Practices**:\n",
    "   - Speak clearly and at a natural pace\n",
    "   - Wait for the agent to complete its response before speaking\n",
    "   - Keep background noise to a minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356a2a40-5280-45da-8850-39e52bbb515c",
   "metadata": {
    "height": 96
   },
   "outputs": [],
   "source": [
    "jupyter.run_app(\n",
    "    WorkerOptions(entrypoint_fnc=entrypoint), \n",
    "    jupyter_url=\"https://jupyter-api-livekit.vercel.app/api/join-token\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e97269-16fd-4b0b-81d6-a0d0c260b6c4",
   "metadata": {},
   "source": [
    "## Step 5: Try new voices\n",
    "Update step 2 with voice id's. For example:  \n",
    "`tts = elevenlabs.TTS(voice_id=\"CwhRBWXzGAHq8TQ4Fs17\") `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4b58e3-0494-4e6a-beff-a5827eaf7c3c",
   "metadata": {
    "height": 81
   },
   "outputs": [],
   "source": [
    "# Available ElevenLabs Voices\n",
    "VOICE_OPTIONS = {\n",
    "    \"Roger\": \"CwhRBWXzGAHq8TQ4Fs17\",  # Professional male voice\n",
    "    \"Sarah\": \"EXAVITQu4vr4xnSDxMaL\",  # Natural female voice\n",
    "    \"Laura\": \"FGY2WhTYpPnrIDTdsKH5\",  # Warm female voice\n",
    "    \"George\": \"JBFqnCBsd6RMkjVDRZzb\"   # Authoritative male voice\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3642eaf-7a6d-45ed-9013-dae64d6ee96f",
   "metadata": {},
   "source": [
    "## Experiment with ElevenLabs:\n",
    "For more information about using Elevenlabs in your voice projects, look for more information at their [website](https://elevenlabs.io/conversational-ai).\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "### Voice AI Integration\n",
    "- [ElevenLabs Documentation](https://elevenlabs.io/docs)\n",
    "- [OpenAI Whisper Guide](https://platform.openai.com/docs/guides/speech-to-text)\n",
    "- [Silero VAD Documentation](https://github.com/snakers4/silero-vad)\n",
    "\n",
    "### Best Practices\n",
    "- For optimal voice interaction, refer to the [ElevenLabs Best Practices](https://elevenlabs.io/docs/best-practices)\n",
    "- For production deployment considerations, see [LiveKit Integration Guide](https://docs.livekit.io/)\n",
    "\n",
    "### Performance Optimization\n",
    "- For latency optimization techniques, refer to the `optimizing_latency.ipynb` notebook\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
